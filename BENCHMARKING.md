
To measure the benefits of the PDX layout you have a few options:
### Using our embeddings
We provide 22 embeddings datasets (~14GB) for you to use (GloVe variants, arXiv, OpenAI, DEEP, GIST, SIFT, MNIST, etc). The `./data/setup_data.py` script will do everything for you (download/unzip data and transform the data to the PDX layout). Edit this script to decide which algorithms you want to use (ADSampling, BOND or BSA). On the `./data/pdx_settings.py` you can decide which datasets specifically to transform.

To run the script:
```shell
pip install -r ./data/requirements.txt
python ./data/setup_data.py
```
The transformed data will be created under the `datasets` directory.
Note that `setup_data.py` may take some time as an IVF index (FAISS IVFFlat) is created for each dataset. Furthermore, ADSampling and (especially) BSA algorithms need to perform  expensive preprocessing on the input data.

### Downloading Data Manually
The `setup_data.py` script will download a .zip with our embeddings from Google Drive (~14GB). If you want to download the vector datasets manually you can find them in this URL: https://drive.google.com/uc?id=1l_5RK28JRL19wpT22B-DY9We3TVXnnQQ

Unzip the file under a directory `./datasets/downloaded`. Then run `python ./data/setup_data.py` to transform and preprocess the data.

### Your own embeddings
*Pending to write*...


## Running Tests
Once you have downloaded and preprocessed the data, you can start benchmarking.

### Building
Requirements:
1. Clang++ (17 or higher)
2. CMake (3.20 or higher)
3. Set CXX variable e.g. `export CXX="/usr/bin/clang++-18"`

Building:
```sh
cmake . -DCMAKE_BUILD_TYPE=Release
make
```

### Compiling to a certain architecture
You will mostly be fine with `-march=native`. However, for Intel SPR one has to manually set the vector width to 512 with `-mprefer-vector-width=512` as LLVM has not yet activated AVX512 by default in this architecture. The latter due to AVX512 downclocking the CPU on earlier Intel archs (Ice Lake and earlier). This is not the case anymore.
```sh
# GRAVITON4
cmake . -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS_RELEASE="-O3 -mcpu=neoverse-v2"
# GRAVITON3
cmake . -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS_RELEASE="-O3 -mcpu=neoverse-v1"
# Intel Sapphire Rapids
cmake . -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS_RELEASE="-O3 -march=sapphirerapids -mtune=sapphirerapids -mprefer-vector-width=512"
# ZEN4
cmake . -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS_RELEASE="-O3 -march=znver4 -mtune=znver4"
# ZEN3
cmake . -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS_RELEASE="-O3 -march=znver3 -mtune=znver3"
# Apple Mac
cmake . -DCMAKE_BUILD_TYPE=Release -DCMAKE_CXX_FLAGS_RELEASE="-O3 -march=native"
```

### Bench All
In the `benchmark_all.sh` you can set additional flags for your CPU architecture. Inside you will find some for the most common ones (Zen4, Zen3, Intel, Sapphire Rapids, Graviton4, Graviton3, M1). Here you can also set a custom path for CXX and Python.
```sh
chmod -R 777 ./benchmarks/benchmark_all.sh
./benchmarks/benchmark_all.sh
```

### Search an existing IVF Index

- Linear Scan: `./benchmarks/BenchmarkHorIVFLinearScan <dataset_name> <buckets_nprobe>`
- ADSampling: `./benchmarks/BenchmarkHorIVFADSampling <dataset_name> <buckets_nprobe>`
- BSA: `./benchmarks/BenchmarkHorIVFBSA <dataset_name> <buckets_nprobe>`
- PDX ADSampling: `./benchmarks/BenchmarkPDXADSampling <dataset_name> <buckets_nprobe>`
- PDX BSA: `./benchmarks/BenchmarkPDXBSA <dataset_name> <buckets_nprobe>`
- PDX BOND: `./benchmarks/BenchmarkPDXIVFBOND <dataset_name> <buckets_nprobe>`

The available `dataset_name`'s and `buckets_nprobe` are configurable in the `/include/benchmark_utils.cpp` file.

*Recall that the IVF index is created by the `setup_data.py` script*

### Linear Scan (Exact Search)
PDX BOND can do exact search faster than other systems if the data is under the PDX layout.
- PDX BOND: ```./benchmarks/BenchmarkPDXBOND```
- Usearch: ```python3 ./benchmarks/bench_systems/exact_usearch.py```
- SKLearn: ```python3 ./benchmarks/bench_systems/exact_sklearn.py```
- Milvus: ```python3 ./benchmarks/bench_systems/exact_milvus.py```
- FAISS: ```python3 ./benchmarks/bench_systems/exact_faiss.py```
    - Note that FAISS precompute the input data squares and norms to be fast. Even so, PDX BOND is faster.

PDX BOND data is generated by the `setup_data.py` script. Usearch, SKLearn, Milvus and FAISS expects the original `.hdf5` files under the `/downloaded` directory.

Usearch, SKLearn, Milvus and FAISS require the respective python packages:
```sh
pip install -r ./benchmarks/bench_systems/requirements.txt
```